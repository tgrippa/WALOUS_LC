{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong><font size=\"6\">WALOUS</font></strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong><font size=\"6\">Postprocessing - Resampling 1m Vectorisation etc..</font></strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This python code implement the method developed by ANAGEO (ULB). \n",
    "\n",
    "Code developped on Linux Mint 18.1 (Ubuntu Xenial 16.04) and GRASS GIS 7.3.svn (r71315)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is a Javascript section of code for building the Jupyter notebook's table of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define working environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries needed for setting parameters of operating system \n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import tempfile\n",
    "import subprocess\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Import multiprocessing and functools libraries\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Add folder with SCR provided belong to this notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add local module to the path\n",
    "src = os.path.abspath('../SRC')\n",
    "if src not in sys.path:\n",
    "    sys.path.append(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Setup environment variables for TAIS DESKTOP (Linux Mint + GRASS Dev) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please edit the file in `../SRC/config.py`, containing the configuration parameters, according to your own computer setup. The following cell is used to run this file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run ../SRC/config_postprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'list_tiles': '../../../Postprocess_V1/list_tiles', 'outputfolder_Logfile': '../../../Postprocess_V1/Log_file', 'permanent_mapset': 'PERMANENT', 'locationepsg': '31370', 'outputfolder': '../../../Postprocess_V1', 'outputfolder_Vecteur': '../../../Postprocess_V1/Vecteur', 'gisdb': '../../GRASSDATA', 'location': 'WALOUS_31370', 'PYTHONLIB': '/usr/bin/python2', 'njobs': 6, 'outputfolder_Raster': '../../../Postprocess_V1/Raster', 'GISBASE': '/usr/lib/grass76'}\n"
     ]
    }
   ],
   "source": [
    "print config_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import functions that setup the environmental variables\n",
    "import environ_variables as envi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDMSESSION = mate \t\n",
      "MANDATORY_PATH = /usr/share/gconf/mate.mandatory.path \t\n",
      "MATE_DESKTOP_SESSION_ID = this-is-deprecated \t\n",
      "LESSOPEN = | /usr/bin/lesspipe %s \t\n",
      "MDM_LANG = fr_BE.UTF-8 \t\n",
      "LOGNAME = tais \t\n",
      "USER = tais \t\n",
      "HOME = /home/tais \t\n",
      "XDG_VTNR = 8 \t\n",
      "PATH = /usr/local/bin:/home/tais/BIN:/home/tais/bin:/home/tais/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/grass76/bin:/usr/lib/grass76/script:/usr/lib/grass76/lib \t\n",
      "CLICOLOR = 1 \t\n",
      "DISPLAY = :0.0 \t\n",
      "SSH_AGENT_PID = 2054 \t\n",
      "LANG = fr_BE.UTF-8 \t\n",
      "TERM = xterm-color \t\n",
      "SHELL = /bin/bash \t\n",
      "GIS_LOCK = $$ \t\n",
      "XAUTHORITY = /home/tais/.Xauthority \t\n",
      "SESSION_MANAGER = local/tais-HP-Z620-Workstation:@/tmp/.ICE-unix/1983,unix/tais-HP-Z620-Workstation:/tmp/.ICE-unix/1983 \t\n",
      "SHLVL = 1 \t\n",
      "QT_LINUX_ACCESSIBILITY_ALWAYS_ON = 1 \t\n",
      "INSIDE_CAJA_PYTHON =  \t\n",
      "QT_ACCESSIBILITY = 1 \t\n",
      "LD_LIBRARY_PATH = :/usr/lib/grass76/lib \t\n",
      "COMPIZ_CONFIG_PROFILE = mate \t\n",
      "WINDOWPATH = 8 \t\n",
      "GTK_OVERLAY_SCROLLING = 0 \t\n",
      "PYTHONPATH = :/usr/lib/grass76/etc/python:/usr/lib/grass76/etc/python/grass:/usr/lib/grass76/etc/python/grass/script \t\n",
      "GISBASE = /usr/lib/grass76 \t\n",
      "CLUTTER_BACKEND = x11 \t\n",
      "USERNAME = tais \t\n",
      "XDG_SESSION_DESKTOP = mate \t\n",
      "GDM_XSERVER_LOCATION = local \t\n",
      "XDG_RUNTIME_DIR = /run/user/1000 \t\n",
      "JPY_PARENT_PID = 18738 \t\n",
      "QT_STYLE_OVERRIDE = gtk \t\n",
      "SSH_AUTH_SOCK = /run/user/1000/keyring/ssh \t\n",
      "VTE_VERSION = 4205 \t\n",
      "GDMSESSION = mate \t\n",
      "GISRC = /home/tais/.grass7/rc \t\n",
      "GIT_PAGER = cat \t\n",
      "XDG_CONFIG_DIRS = /etc/xdg/xdg-mate:/etc/xdg \t\n",
      "XDG_CURRENT_DESKTOP = MATE \t\n",
      "XDG_SESSION_ID = c1 \t\n",
      "DBUS_SESSION_BUS_ADDRESS = unix:abstract=/tmp/dbus-x9ruYoZFLw,guid=6011becff9b6b6fb7cb7db855d78d34b \t\n",
      "_ = /usr/local/bin/jupyter \t\n",
      "XDG_SESSION_COOKIE = 8441891e86e24d76b9616edf516d5734-1568199499.139047-768758797 \t\n",
      "DESKTOP_SESSION = mate \t\n",
      "WINDOWID = 106954758 \t\n",
      "LESSCLOSE = /usr/bin/lesspipe %s %s \t\n",
      "DEFAULTS_PATH = /usr/share/gconf/mate.default.path \t\n",
      "MPLBACKEND = module://ipykernel.pylab.backend_inline \t\n",
      "MDM_XSERVER_LOCATION = local \t\n",
      "GTK_MODULES = gail:atk-bridge \t\n",
      "XDG_DATA_DIRS = /usr/share/mate:/usr/local/share/:/usr/share/:/usr/share/mdm/ \t\n",
      "PWD = /media/tais/data/WALOUS/Landcover/Processing/GithubRepository_WALOUS \t\n",
      "COLORTERM = mate-terminal \t\n",
      "PYTHONLIB = /usr/bin/python2 \t\n",
      "LS_COLORS = rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36: \t\n",
      "PAGER = cat \t\n",
      "XDG_SEAT = seat0 \t\n"
     ]
    }
   ],
   "source": [
    "# Set environmental variables\n",
    "envi.setup_environmental_variables() \n",
    "# Display current environment variables of your computer\n",
    "envi.print_environmental_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** GRASS GIS Python libraries **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries needed to launch GRASS GIS in the jupyter notebook\n",
    "import grass.script.setup as gsetup\n",
    "# Import libraries needed to call GRASS using Python\n",
    "import grass.script as gscript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import function that check and create folder\n",
    "from mkdir import check_create_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import function that generate a random name in the GRASS GIS environement\n",
    "from random_layer_name import random_layer_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import function that check existance and create GRASS GIS database folder if needed\n",
    "from grass_database import check_gisdb, check_location, check_mapset, working_mapset\n",
    "# Import functions for processing time information\n",
    "from processing_time import start_processing, print_processing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def launch_mapset(mapset):\n",
    "    #Declare empty list that will contain the messages to return\n",
    "    return_message = []\n",
    "    # Check if the location exists and create it if not, with the CRS defined by the epsg code \n",
    "    return_message.append(check_location(config_parameters[\"gisdb\"],config_parameters['location'],config_parameters[\"locationepsg\"]))\n",
    "    # Check if mapset exists\n",
    "    return_message.append(check_mapset(config_parameters[\"gisdb\"],config_parameters['location'],mapset))\n",
    "    # Change the current working GRASS GIS session mapset\n",
    "    return_message.append(working_mapset(config_parameters[\"gisdb\"],config_parameters['location'],mapset))\n",
    "    # Return\n",
    "    return return_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetMapsetsAccess():\n",
    "    #Declare empty string that will contain the messages to return\n",
    "    return_message = ''\n",
    "    try:\n",
    "        # Add mapsets with input data to the GRASS GIS research path\n",
    "        gscript.run_command('g.mapsets', mapset=\"FUSIONS\", operation=\"add\")\n",
    "        return_message = \"Access to other mapset added\"\n",
    "    except:\n",
    "        return_message += \"ERROR: Add access to other Mapsets failed. Please check for problem.\"\n",
    "    return return_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DefineComputationRegionAndMask(tile_cat,resamp_resolution='1', grow_mmu='16'):\n",
    "    #Declare empty string that will contain the messages to return\n",
    "    return_message = ''\n",
    "    try:\n",
    "        return_message = \"Working on tile '%s'\\n\"%tile_cat\n",
    "        # 'Grow' parameter is used to expend the computational region ouside of the current tile to deal with bordering objects smaller than MMU\n",
    "        gscript.run_command('g.region', flags='a', raster='RF_fusion_tile_%s'%tile_cat, \n",
    "                            res=resamp_resolution, grow=grow_mmu)\n",
    "        # Define MASK for clipping at the end\n",
    "        gscript.run_command('r.mask', overwrite=True, raster='RF_fusion_tile_%s'%tile_cat) \n",
    "        gscript.run_command('g.copy', overwrite=True, raster='MASK,MASK_tile25') #25cm resolution mask following the original limit of the tile\n",
    "        gscript.run_command('r.mask', flags='r') #Remove mask\n",
    "        # Define MASK for processing (overlapping with the neighboring tiles)\n",
    "        gscript.run_command('r.buffer', overwrite=True, input='MASK_tile25', \n",
    "                            output='MASK_tmp', distances=grow_mmu)\n",
    "        rulefile = gscript.tempfile()\n",
    "        with open(rulefile, 'w') as f:\n",
    "            f.write('2 = 1\\n* = *') #Reclass rule\n",
    "        gscript.run_command('r.reclass', overwrite=True, input='MASK_tmp', output='MASK_processing', rules=rulefile)\n",
    "        # Print\n",
    "        return_message += \"--> Setting of computational region and masks succeeded.\"\n",
    "    except:\n",
    "        return_message += \"ERROR: Setting of computional region and masks failed for cutline '%s'. Please check for problem.\"%tile_cat\n",
    "    return return_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Resamp(tile_cat):\n",
    "    #Resample classification \n",
    "    return_message = ''\n",
    "    try:\n",
    "        gscript.run_command('r.mask', overwrite=True, raster='MASK_processing') \n",
    "        gscript.run_command('r.resamp.stats', overwrite=True, input='fusion_lc', \n",
    "                            output='resamp', method='mode')\n",
    "        return_message += \"--> Resampling succeeded.\"\n",
    "    except:\n",
    "        return_message += \"ERROR: Resampling for cutline '%s' failed. Please check for problem.\"%tile_cat\n",
    "    return return_message   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Function to generate and execute an SQL insert query based on input 'table_name','head' and 'value_dict'. \n",
    "The key of the dictionnary 'value_dict' should will be the first column of the table (unique id) and the values should\n",
    "contain a list with the values (the other columns). Please be sure that len('head') == 1+len(value_dict[key])\n",
    "\"\"\"\n",
    "def SqlInsert(table_name, header, value_dict, overwrite):\n",
    "    sql_query = gscript.tempfile()\n",
    "    fsql = open(sql_query, 'w')\n",
    "    fsql.write('BEGIN TRANSACTION;\\n')\n",
    "    if gscript.db_table_exist(table_name):\n",
    "        if overwrite:\n",
    "            fsql.write('DROP TABLE %s;\\n' % table_name)\n",
    "        else:\n",
    "            gscript.fatal(_(\"Table %s already exists. Use 'overwrite=True' to overwrite\" % table_name))\n",
    "    create_statement = 'CREATE TABLE ' + table_name + ' (cat int PRIMARY KEY);\\n'\n",
    "    fsql.write(create_statement)\n",
    "    for col in header[1:]:\n",
    "        if col.split('_')[-1] == 'cat':  # Mode column should be integer\n",
    "            addcol_statement = 'ALTER TABLE %s ADD COLUMN %s integer;\\n' % (table_name, col)\n",
    "        else: # Proportions column should be double precision\n",
    "            addcol_statement = 'ALTER TABLE %s ADD COLUMN %s double precision;\\n' % (table_name, col)\n",
    "        fsql.write(addcol_statement)\n",
    "    for key in value_dict:\n",
    "            insert_statement = 'INSERT INTO %s VALUES (%s, %s);\\n' % (table_name, key, ','.join([str(x) for x in value_dict[key]]))\n",
    "            fsql.write(insert_statement)\n",
    "    fsql.write('END TRANSACTION;')\n",
    "    fsql.close()\n",
    "    gscript.run_command('db.execute', input=sql_query, quiet=True)\n",
    "\n",
    "'''\n",
    "Function to compute shape statistics for each clump and import it as table in GRASS GIS\n",
    "'''\n",
    "def GetClumpShapeStatistics(overwrite=True):\n",
    "    # Path to temporary file for 'r.object.geometry' output\n",
    "    tmp_csv = \"%s_robjectgeometry\"%gscript.tempfile()\n",
    "    # Compute shape statistics for each clump\n",
    "    gscript.run_command('r.object.geometry', overwrite=True, flags='m', input='clump', output=tmp_csv)\n",
    "    # Load csv content in python dictionnary\n",
    "    incsv = open(tmp_csv, 'r')\n",
    "    reader = csv.reader(incsv, delimiter='|')\n",
    "    header = reader.next()\n",
    "    value_dict = {row[0]:row[1:] for row in reader}\n",
    "    incsv.close()\n",
    "    # Insert SQL\n",
    "    table_name = \"geom\"\n",
    "    SqlInsert(table_name, header, value_dict, overwrite)\n",
    "\n",
    "'''\n",
    "Function to retrieve the land cover class of each clump and import it as table in GRASS GIS\n",
    "'''\n",
    "def GetClumpLCClass(overwrite=True):\n",
    "    # Path to temporary file for 'r.object.geometry' output\n",
    "    tmp_csv = \"%s_runivar\"%gscript.tempfile()\n",
    "    # Compute r.univar to get class label of the clump\n",
    "    gscript.run_command('r.univar', overwrite=True, flags='t', map='resamp', zones='clump', output=tmp_csv)\n",
    "    # Load csv content in python dictionnary\n",
    "    incsv = open(tmp_csv, 'r')\n",
    "    reader = csv.reader(incsv, delimiter='|')\n",
    "    reader.next() # Skip the first row (header)\n",
    "    header = [\"cat\",\"label\"] # Create a header with custom column name\n",
    "    value_dict = {row[0]:row[4:5] for row in reader} #Take only the column corresponding to 'min'\n",
    "    incsv.close()\n",
    "    # Insert SQL\n",
    "    table_name = \"label\"\n",
    "    SqlInsert(table_name, header, value_dict, overwrite)\n",
    "\n",
    "'''\n",
    "Function to retrieve the neighborhood of each clump and import it as table in GRASS GIS\n",
    "'''\n",
    "def GetClumpNeighbor(overwrite=True):\n",
    "    # Path to temporary file for 'r.object.geometry' output\n",
    "    tmp_csv = \"%s_neighbor\"%gscript.tempfile()\n",
    "    # Compute neighborhood matrix\n",
    "    gscript.run_command('r.neighborhoodmatrix', overwrite=True, flags='l', input='clump', output=tmp_csv)\n",
    "    # Load csv content in python dictionnary\n",
    "    incsv = open(tmp_csv, 'r')\n",
    "    reader = csv.reader(incsv, delimiter='|')\n",
    "    header = [\"cat\",\"sega\",\"segb\",\"border_count\"] # Create a header\n",
    "    value_dict = {id_:row[:] for id_,row in enumerate(reader)} #Take only the column corresponding to 'min'\n",
    "    incsv.close()\n",
    "    # Insert SQL\n",
    "    table_name = \"rmatrix\"\n",
    "    SqlInsert(table_name, header, value_dict, overwrite)\n",
    "\n",
    "\"\"\" \n",
    "Function to pivot rmatrix table to horizontal table with proportion to each class\n",
    "\"\"\"\n",
    "def GetNeighborClassProportion(overwrite=True):\n",
    "    global labels_list\n",
    "    ### Table 'tmp_a'\n",
    "    table_name = \"tmp_a\"\n",
    "    sql_query = gscript.tempfile()\n",
    "    fsql = open(sql_query, 'w')\n",
    "    fsql.write('BEGIN TRANSACTION;\\n')\n",
    "    if gscript.db_table_exist(table_name):\n",
    "        if overwrite:\n",
    "            fsql.write('DROP TABLE %s;\\n'%table_name)\n",
    "        else:\n",
    "            gscript.fatal(_(\"Table %s already exists. Use 'overwrite=True' to overwrite\"%table_name))\n",
    "    create_statement = 'CREATE TABLE %s AS '%table_name\n",
    "    create_statement += 'SELECT a.sega, b.label, border_count FROM rmatrix AS a '\n",
    "    create_statement += 'LEFT JOIN label AS b ON a.segb=b.cat;\\n'\n",
    "    fsql.write(create_statement)\n",
    "    fsql.write('END TRANSACTION;')\n",
    "    fsql.close()\n",
    "    gscript.run_command('db.execute', input=sql_query, quiet=True)\n",
    "    \n",
    "    ### Table 'tmp_b'\n",
    "    table_name = \"tmp_b\"\n",
    "    sql_query = gscript.tempfile()\n",
    "    fsql = open(sql_query, 'w')\n",
    "    fsql.write('BEGIN TRANSACTION;\\n')\n",
    "    if gscript.db_table_exist(table_name):\n",
    "        if overwrite:\n",
    "            fsql.write('DROP TABLE %s;\\n'%table_name)\n",
    "        else:\n",
    "            gscript.fatal(_(\"Table %s already exists. Use 'overwrite=True' to overwrite\"%table_name))\n",
    "    create_statement = 'CREATE TABLE %s AS WITH '%table_name\n",
    "    create_statement += 'border_lenght AS (SELECT sega, sum(border_count) AS sum_border FROM tmp_a GROUP BY sega),'\n",
    "    create_statement += 'tempotable AS (SELECT a.sega, a.label, round((a.border_count*1.0/b.sum_border*1.0),8) AS percent_border FROM tmp_a AS a LEFT JOIN border_lenght AS b ON a.sega=b.sega)'\n",
    "    create_statement += 'SELECT sega AS seg, label, sum(percent_border) AS percent_border FROM tempotable GROUP BY sega, label ORDER BY sega, label;\\n'\n",
    "    create_statement += 'UPDATE %s SET percent_border=1.0 WHERE percent_border>0.9999;\\n'%table_name\n",
    "    create_statement += 'UPDATE %s SET percent_border=0.0 WHERE percent_border<0.0001;\\n'%table_name\n",
    "    fsql.write(create_statement)\n",
    "    fsql.write('END TRANSACTION;')\n",
    "    fsql.close()\n",
    "    gscript.run_command('db.execute', input=sql_query, quiet=True)\n",
    "    \n",
    "    ### Table 'pivot_prop_label'\n",
    "    table_name = \"pivot_prop_label\"\n",
    "    # Get distinct values for 'label' class\n",
    "    distinctlabelquery=\"SELECT DISTINCT label FROM tmp_b ORDER BY 1\"\n",
    "    labels_list = [x[0] for x in gscript.db_select(distinctlabelquery)] #According to classes that exist in the table\n",
    "#    labels_list = ['2','3','5','11','12','41','42','61','62] #Forcing according to classes that should be included in the table\n",
    "    # Declaration of colums for the pivot table\n",
    "    columns=[\"seg INTEGER\",]\n",
    "    [columns.append(\"prop_%s NUMERIC\"%label) for label in labels_list]\n",
    "    # Crosstab query argument\n",
    "    crosstabquery = \"SELECT seg, label, percent_border FROM tmp_b ORDER  BY 1,2\"\n",
    "    # Built complete sql query\n",
    "    sql_query = gscript.tempfile()\n",
    "    fsql = open(sql_query, 'w')\n",
    "    fsql.write('BEGIN TRANSACTION;\\n')\n",
    "    if gscript.db_table_exist(table_name):\n",
    "        if overwrite:\n",
    "            fsql.write('DROP TABLE %s;\\n'%table_name)\n",
    "        else:\n",
    "            gscript.fatal(_(\"Table %s already exists. Use 'overwrite=True' to overwrite\"%table_name))\n",
    "    # Create statement for the pivot table\n",
    "    pivot_statement = \"SELECT seg, \"\n",
    "    pivot_statement += ', '.join([\"SUM(CASE WHEN label = '%s' THEN percent_border END) AS prop_%s\"%(cl,cl) for cl in labels_list])\n",
    "    pivot_statement += \" FROM tmp_b GROUP BY seg\"\n",
    "    create_statement = \"CREATE TABLE %s AS %s;\\n\"%(table_name,pivot_statement)\n",
    "    fsql.write(create_statement)\n",
    "    # List of update queries to update each column\n",
    "    [fsql.write(\"UPDATE %s SET prop_%s=0.0 WHERE prop_%s is null;\\n\"%(table_name,label,label)) for label in labels_list]\n",
    "    fsql.write('END TRANSACTION;')\n",
    "    fsql.close()\n",
    "    gscript.run_command('db.execute', input=sql_query, quiet=True)\n",
    "\n",
    "\"\"\" \n",
    "Function to \n",
    "\"\"\"\n",
    "\n",
    "def AddLabelNthNeighbor(overwrite=True):\n",
    "    # The trick was found here: https://stackoverflow.com/questions/8436919/second-maximum-and-minimum-values\n",
    "    # To be able to run the command directly in GRASS, be sure the version of SQlite used in compilation is >3.25\n",
    "    # The function used here is available only for recent version https://stackoverflow.com/questions/50332436/syntax-error-when-using-row-number-in-sqlite3r\n",
    "    # Get the largest, second largest and third largest value\n",
    "    ### SQL query\n",
    "    table_name = \"rank_prop\"\n",
    "    sql_query = gscript.tempfile()\n",
    "    fsql = open(sql_query, 'w')\n",
    "    fsql.write('BEGIN TRANSACTION;\\n')\n",
    "    if gscript.db_table_exist('tmp_c'):\n",
    "        if overwrite:\n",
    "            fsql.write(\"DROP TABLE tmp_c;\\n\")\n",
    "        else:\n",
    "            gscript.fatal(_(\"Table 'tmp_c' already exists. Use 'overwrite=True' to overwrite\"))\n",
    "    subquery = \"SELECT seg, percent_border, row_number() OVER ( \"\n",
    "    subquery += \"PARTITION BY seg order by percent_border DESC) as rnDesc FROM tmp_B\"\n",
    "    create_statement = \"CREATE TABLE tmp_c AS \"\n",
    "    create_statement += 'SELECT seg, '\n",
    "    create_statement += 'MAX(CASE when rnDesc = 1 THEN percent_border END) as First_prop, '\n",
    "    create_statement += 'MAX(CASE when rnDesc = 2 THEN percent_border END) as Second_prop, '\n",
    "    create_statement += 'MAX(CASE when rnDesc = 3 THEN percent_border END) as Third_prop '\n",
    "    create_statement += 'from(%s) as SubQueryAlias GROUP BY seg;\\n'%subquery\n",
    "    fsql.write(create_statement)\n",
    "\n",
    "    if gscript.db_table_exist(table_name):\n",
    "        if overwrite:\n",
    "            fsql.write(\"DROP TABLE %s;\\n\"%table_name)\n",
    "        else:\n",
    "            gscript.fatal(_(\"Table '%s' already exists. Use 'overwrite=True' to overwrite\"%table_name))\n",
    "    create_statement = \"CREATE TABLE %s AS \"%table_name\n",
    "    create_statement += 'SELECT a.*,b.First_prop,b.Second_prop,b.Third_prop FROM pivot_prop_label AS a LEFT JOIN tmp_C AS b ON a.seg=b.seg;\\n'\n",
    "    fsql.write(create_statement)\n",
    "    alter_statement = \"ALTER TABLE %s ADD COLUMN first_label integer;\\n\"%table_name\n",
    "    alter_statement += \"ALTER TABLE %s ADD COLUMN second_label integer;\\n\"%table_name\n",
    "    alter_statement += \"ALTER TABLE %s ADD COLUMN third_label integer;\\n\"%table_name\n",
    "    fsql.write(alter_statement)   \n",
    "    update_queries = []\n",
    "    for label in labels_list:\n",
    "        update_queries.append(\"UPDATE {t} SET first_label={l} WHERE first_prop=prop_{l}\".format(t=table_name, l=label))\n",
    "        update_queries.append(\"UPDATE {t} SET second_label={l} WHERE second_prop=prop_{l}\".format(t=table_name, l=label))\n",
    "        update_queries.append(\"UPDATE {t} SET third_label={l} WHERE third_prop=prop_{l}\".format(t=table_name, l=label))\n",
    "    fsql.write(\";\\n\".join(update_queries))\n",
    "    fsql.write(\";\\n\")\n",
    "    fsql.write('END TRANSACTION;')\n",
    "    fsql.close()    \n",
    "    # Bash file\n",
    "    bash_sqlite = gscript.tempfile()\n",
    "    bash = open(bash_sqlite, 'w')\n",
    "    bash.write('sqlite3 ../../GRASSDATA/WALOUS_31370/6326/sqlite/sqlite.db < %s'%sql_query)\n",
    "    bash.close() \n",
    "    try:\n",
    "        subprocess.check_call(['bash', bash_sqlite], stderr=subprocess.STDOUT, )\n",
    "    except subprocess.CalledProcessError:\n",
    "        message =  \"There was an error in the execution of the bash script.\\nPlease check and fix.\"\n",
    "\n",
    "\"\"\" \n",
    "Function to join all information together\n",
    "\"\"\"\n",
    "def GetFinalJoinedTable(overwrite=True):\n",
    "    ### Table 'pivot_final'\n",
    "    table_name = \"pivot_final\"\n",
    "    sql_query = gscript.tempfile()\n",
    "    fsql = open(sql_query, 'w')\n",
    "    fsql.write('BEGIN TRANSACTION;\\n')\n",
    "    if gscript.db_table_exist(table_name):\n",
    "        if overwrite:\n",
    "            fsql.write('DROP TABLE %s;\\n'%table_name)\n",
    "        else:\n",
    "            gscript.fatal(_(\"Table %s already exists. Use 'overwrite=True' to overwrite\"%table_name))\n",
    "    create_statement = 'CREATE TABLE %s AS '%table_name\n",
    "    create_statement += 'SELECT a.cat AS seg, a.label, g.area, p.* FROM label AS a '\n",
    "    create_statement += 'LEFT JOIN geom AS g ON a.cat=g.cat '\n",
    "    create_statement += 'LEFT JOIN rank_prop AS p ON a.cat=p.seg ;\\n'\n",
    "    fsql.write(create_statement)\n",
    "    fsql.write('END TRANSACTION;')\n",
    "    fsql.close()\n",
    "    gscript.run_command('db.execute', input=sql_query, quiet=True)\n",
    "\n",
    "\"\"\" \n",
    "Function to remove all intermediate tables\n",
    "\"\"\"\n",
    "def RemoveIntermediateTables(overwrite=True):\n",
    "    list_tables_to_remove = ['label','geom','rmatrix','tmp_a','tmp_b','tmp_c','pivot_prop_label','rank_prop']\n",
    "    sql_query = gscript.tempfile()\n",
    "    fsql = open(sql_query, 'w')\n",
    "    fsql.write('BEGIN TRANSACTION;\\n')\n",
    "    for table_name in list_tables_to_remove:\n",
    "        if gscript.db_table_exist(table_name):\n",
    "            if overwrite:\n",
    "                fsql.write('DROP TABLE %s;\\n'%table_name)\n",
    "            else:\n",
    "                gscript.fatal(_(\"Table %s already exists. Use 'overwrite=True' to overwrite\"%table_name))\n",
    "    fsql.write('END TRANSACTION;')\n",
    "    fsql.close()\n",
    "    gscript.run_command('db.execute', input=sql_query, quiet=True)\n",
    "    \n",
    "\"\"\" \n",
    "Wrapper function that create clump and compute neighborhood information for reclassification using rules\n",
    "\"\"\"\n",
    "def GetNeighborStat(overwrite=True):\n",
    "    # Define region based on 1m resampled LC\n",
    "    gscript.run_command('g.region', raster='resamp')\n",
    "    # Clump to get unique ID for each LC patch\n",
    "    gscript.run_command('r.clump', overwrite=True, \n",
    "                        flags='d', input='resamp', output='clump')  # flag '-d' to clump also diagonal cells\n",
    "    GetClumpShapeStatistics(overwrite)\n",
    "    GetClumpLCClass(overwrite)\n",
    "    GetClumpNeighbor(overwrite)\n",
    "    GetNeighborClassProportion(overwrite)\n",
    "    AddLabelNthNeighbor(overwrite)\n",
    "    GetFinalJoinedTable(overwrite)\n",
    "    RemoveIntermediateTables(overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReclassAccordingToRule(tile_cat, MMU='0.0015'):\n",
    "    #Resample classification \n",
    "    return_message = ''\n",
    "    try:\n",
    "        gscript.run_command('r.reclass.area', flags='c', overwrite=True, input='resamp',\n",
    "                            output='resamp_mmu', value=MMU, mode='lesser', method='rmarea')\n",
    "        return_message += \"--> Removing area smaller than MMU succeeded.\"\n",
    "    except:\n",
    "        return_message += \"ERROR: Removing area smaller than MMU failed for cutline '%s'. Please check for problem.\"%tile_cat\n",
    "    return return_message   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Vectorize(tile_cat):\n",
    "    #Resample classification \n",
    "    return_message = ''\n",
    "    try:\n",
    "        gscript.run_command('r.to.vect', flags='s', overwrite=True,\n",
    "                            input='resamp_mmu', output='vect', type='area', column='class')\n",
    "        return_message += \"--> Vectorization succeeded.\"\n",
    "    except:\n",
    "        return_message += \"ERROR: Vectorization of raster map failed for cutline '%s'. Please check for problem.\"%tile_cat\n",
    "    return return_message   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def VectorGeneralize(tile_cat, thresh_value=\"10000000\", iter_value=\"3\"):\n",
    "    #Resample classification \n",
    "    return_message = ''\n",
    "    try:\n",
    "        gscript.run_command('v.generalize', overwrite=True, input='vect', type='area', output='vect_generalized', \n",
    "                            method='chaiken', threshold=thresh_value, iterations=iter_value)\n",
    "        return_message += \"--> Smoothing of vector layer succeeded.\"\n",
    "    except:\n",
    "        return_message += \"ERROR: Smoothing of vector layer failed for cutline '%s'. Please check for problem.\"%tile_cat\n",
    "    return return_message   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ClipToTileBorder(tile_cat):\n",
    "    #Resample classification \n",
    "    return_message = ''\n",
    "    try:\n",
    "        gscript.run_command('r.mask', overwrite=True, raster='MASK_tile25') \n",
    "        gscript.run_command('r.clip', overwrite=True, input='resamp_mmu', output='clip_output')\n",
    "        gscript.run_command('r.mask', flags='r') \n",
    "        return_message += \"--> Removing area smaller than MMU succeeded.\"\n",
    "    except:\n",
    "        return_message += \"ERROR: Removing area smaller than MMU failed for cutline '%s'. Please check for problem.\"%tile_cat\n",
    "    return return_message   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ColorizeRaster(tile_cat):\n",
    "    #Declare empty string that will contain the messages to return\n",
    "    return_message = ''\n",
    "    try:\n",
    "        gscript.run_command('r.colors', map='resamp', rules=data['color_file'])\n",
    "        gscript.run_command('r.colors', map='resamp_mmu', rules=data['color_file'])\n",
    "        return_message += \"--> Application of colors table succeeded.\"\n",
    "    except:\n",
    "        return_message += \"ERROR: Application of colors table on raster failed for tile %s. Please check for problem.\"%tile_cat\n",
    "    return return_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ColorizeVector(tile_cat):\n",
    "    #Declare empty string that will contain the messages to return\n",
    "    return_message = ''\n",
    "    try:\n",
    "        gscript.run_command('v.colors', map='vect_generalized', use='attr',\n",
    "                            column='class', rules=data['color_file'])\n",
    "        return_message += \"--> Application of colors table for vector layer succeeded.\"\n",
    "    except:\n",
    "        return_message += \"ERROR: Application of colors table on vector failed for tile %s. Please check for problem.\"%tile_cat\n",
    "    return return_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ExportFinalRast(tile_cat):\n",
    "    global list_rast_fusion\n",
    "    #Declare empty string that will contain the messages to return\n",
    "    return_message = ''\n",
    "    try:\n",
    "        export_path = os.path.join(config_parameters['outputfolder_Raster'],\"LC_output_tile_%s.tif\"%tile_cat)\n",
    "        # Export the group as a .tif file\n",
    "        gscript.run_command('r.out.gdal', quiet=True, overwrite=True, input=\"clip_output\", output=export_path,\n",
    "                            format='GTiff', createopt='COMPRESS=DEFLATE') #Use flag c to not export colortable. Flag m to not export non-standard format of meta-data\n",
    "        return_message += \"--> Export of raster suceeded.\"\n",
    "    except:\n",
    "        return_message += \"ERROR: Export of raster failed for cutline '%s'. Please check for problem.\"%tile_cat\n",
    "    return return_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ExportIntermediateRast(tile_cat):\n",
    "    global list_rast_fusion\n",
    "    #Declare empty string that will contain the messages to return\n",
    "    return_message = ''\n",
    "    try:\n",
    "        export_path = os.path.join(config_parameters['outputfolder_Raster'],\"LC_resamp_tile_%s.tif\"%tile_cat)\n",
    "        # Export the group as a .tif file\n",
    "        gscript.run_command('r.out.gdal', quiet=True, overwrite=True, input=\"resamp\", output=export_path,\n",
    "                            format='GTiff', createopt='COMPRESS=DEFLATE') #Use flag c to not export colortable. Flag m to not export non-standard format of meta-data\n",
    "        export_path = os.path.join(config_parameters['outputfolder_Raster'],\"LC_resamp_mmu_tile_%s.tif\"%tile_cat)\n",
    "        gscript.run_command('r.out.gdal', quiet=True, overwrite=True, input=\"resamp_mmu\", output=export_path,\n",
    "                            format='GTiff', createopt='COMPRESS=DEFLATE') #Use flag c to not export colortable. Flag m to not export non-standard format of meta-data\n",
    "        return_message += \"--> Export of raster suceeded.\"\n",
    "    except:\n",
    "        return_message += \"ERROR: Export of raster failed for cutline '%s'. Please check for problem.\"%tile_cat\n",
    "    return return_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ExportVect(tile_cat):\n",
    "    global list_rast_fusion\n",
    "    #Declare empty string that will contain the messages to return\n",
    "    return_message = ''\n",
    "    try:\n",
    "        export_path = os.path.join(config_parameters['outputfolder_Vecteur'],\"LC_resamp_mmu_tile_%s.gpkg\"%tile_cat)\n",
    "        # Export the group as a .tif file\n",
    "        gscript.run_command('v.out.ogr', quiet=True, overwrite=True,  input=\"vect_generalized\",\n",
    "                            output=export_path, format='GPKG') #Use flag c to not export colortable. Flag m to not export non-standard format of meta-data\n",
    "        return_message += \"--> Export of vector suceeded.\"\n",
    "    except:\n",
    "        return_message += \"ERROR: Export of vector failed for cutline '%s'. Please check for problem.\"%tile_cat\n",
    "    return return_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ExportAllVect(tile_cat):\n",
    "    global list_rast_fusion\n",
    "    #Declare empty string that will contain the messages to return\n",
    "    return_message = ''\n",
    "    try:\n",
    "        export_path = os.path.join(config_parameters['outputfolder_Vecteur'],\"LC_resamp_mmu_vect_tile_%s.gpkg\"%tile_cat)\n",
    "        # Export the group as a .tif file\n",
    "        gscript.run_command('v.out.ogr', quiet=True, overwrite=True,  input=\"vect\",\n",
    "                            output=export_path, format='GPKG') #Use flag c to not export colortable. Flag m to not export non-standard format of meta-data\n",
    "        export_path = os.path.join(config_parameters['outputfolder_Vecteur'],\"LC_resamp_mmu_vect_smooth_tile_%s.gpkg\"%tile_cat)\n",
    "        # Export the group as a .tif file\n",
    "        gscript.run_command('v.out.ogr', quiet=True, overwrite=True,  input=\"vect_generalized\",\n",
    "                            output=export_path, format='GPKG') #Use flag c to not export colortable. Flag m to not export non-standard format of meta-data\n",
    "        return_message += \"--> Export of vector suceeded.\"\n",
    "    except:\n",
    "        return_message += \"ERROR: Export of vector failed for cutline '%s'. Please check for problem.\"%tile_cat\n",
    "    return return_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Clean(tile_cat):\n",
    "    #Declare empty string that will contain the messages to return\n",
    "    return_message = ''\n",
    "    try:\n",
    "        for layer in list_rast_fusion:\n",
    "            gscript.run_command('g.remove', flags='f', type=\"raster\", name=layer)\n",
    "        return_message += \"--> Mapset cleaned\"\n",
    "    except:\n",
    "        return_message += \"ERROR: during mapset cleaning. Please check for problem.\"\n",
    "    return return_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Worker(tile_cat):\n",
    "    import subprocess\n",
    "    start_tile = start_processing() \n",
    "#    print \"Start processing on tile %s\"%tile_cat\n",
    "    #Declare empty list for saving output messages\n",
    "    output_message = [] \n",
    "    \n",
    "    # Launch mapset\n",
    "    message = launch_mapset(tile_cat)  \n",
    "    [output_message.append(a) for a in message]\n",
    "#    print \"\\n\".join(message)\n",
    "\n",
    "    # Allow access to other mapset \n",
    "    message = GetMapsetsAccess()\n",
    "    output_message.append(message)\n",
    "#    print message\n",
    "    \n",
    "    # Define computional region and mask\n",
    "    message = DefineComputationRegionAndMask(tile_cat,resamp_resolution='1', grow_mmu='16')\n",
    "    output_message.append(message)\n",
    "#    print message    \n",
    "\n",
    "    # Resample\n",
    "    message = Resamp(tile_cat)\n",
    "    output_message.append(message)\n",
    "#    print message \n",
    "\n",
    "    # GetNeighborStat\n",
    "    message = GetNeighborStat(overwrite=True)\n",
    "    output_message.append(message)\n",
    "#    print message \n",
    "\n",
    "    # Reclass clumps according to rule \n",
    "    #message = ReclassAccordingToRule(tile_cat, MMU='0.0015')\n",
    "    #output_message.append(message)\n",
    "#    print message \n",
    "\n",
    "    # ColorizeRaster\n",
    "    message = ColorizeRaster(tile_cat)\n",
    "    output_message.append(message)\n",
    "#    print message \n",
    "\n",
    "    # Vectorize\n",
    "    message = Vectorize(tile_cat)\n",
    "    output_message.append(message)\n",
    "#    print message \n",
    "\n",
    "    # VectorGeneralize\n",
    "    message = VectorGeneralize(tile_cat, thresh_value=\"10000000\", iter_value=\"3\")\n",
    "    output_message.append(message)\n",
    "#    print message \n",
    "\n",
    "    # ClipToTileBorder\n",
    "    message = ClipToTileBorder(tile_cat)\n",
    "    output_message.append(message)\n",
    "#    print message \n",
    "\n",
    "    # ColorizeVector\n",
    "    message = ColorizeVector(tile_cat)\n",
    "    output_message.append(message)\n",
    "#    print message \n",
    "\n",
    "    # ExportRast \n",
    "    message = ExportFinalRast(tile_cat)\n",
    "    output_message.append(message)\n",
    "#    print message\n",
    "    \n",
    "    # ExportIntermediateRast \n",
    "    message = ExportIntermediateRast(tile_cat)\n",
    "    output_message.append(message)\n",
    "#    print message\n",
    "\n",
    "    # ExportVect \n",
    "    #message = ExportVect(tile_cat)\n",
    "    #output_message.append(message)\n",
    "#    print message\n",
    "\n",
    "    # ExportAllVect \n",
    "    message = ExportAllVect(tile_cat)\n",
    "    output_message.append(message)\n",
    "#    print message\n",
    "\n",
    "    # Clean \n",
    "    #message = Clean(tile_cat)\n",
    "    #output_message.append(message)\n",
    "#    print message\n",
    "\n",
    "    #Print processing time\n",
    "    message = print_processing_time(start_tile, \"Prediction for tile '%s' achieved in \"%tile_cat)\n",
    "    output_message.append(message)\n",
    "#    print message\n",
    "    \n",
    "    #Export Log file\n",
    "    fout = open(os.path.join(config_parameters['outputfolder_Logfile'],\"Log_Prediction_tile_%s.txt\"%tile_cat),\"w\")\n",
    "    [fout.writelines('%s\\n'%content) for content in output_message]\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder '../../../Postprocess_V1' already exists\n",
      "The folder '../../../Postprocess_V1/Log_file' already exists\n",
      "The folder '../../../Postprocess_V1/Raster' already exists\n",
      "The folder '../../../Postprocess_V1/Vecteur' already exists\n"
     ]
    }
   ],
   "source": [
    "# Check and create folder if needed\n",
    "check_create_dir(config_parameters['outputfolder'])\n",
    "check_create_dir(config_parameters['outputfolder_Logfile'])\n",
    "check_create_dir(config_parameters['outputfolder_Raster'])\n",
    "check_create_dir(config_parameters['outputfolder_Vecteur'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import fusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run ../SRC/config_postprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Location 'WALOUS_31370' already exist\",\n",
       " \"'FUSIONS' mapset already exists in location 'WALOUS_31370'\",\n",
       " \"You are now working in mapset 'WALOUS_31370/FUSIONS'\"]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create mapset \n",
    "launch_mapset(\"FUSIONS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 375 paths in the list\n"
     ]
    }
   ],
   "source": [
    "# Create a list of paths to files with a specific name prefix\n",
    "list_file = glob.glob(os.path.join(data['fusion_folder'],\"RF_fusion_tile_*.tif\"))\n",
    "print \"There are %s paths in the list\"%len(list_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get list of cat that are included in the shapefile with tiles to be processed\n",
    "import geopandas as gpd\n",
    "gdf = gpd.read_file(data['tiles'][1])\n",
    "tiles_aoi = list(gdf.cat)\n",
    "gdf = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 TIF files will be imported (contained in the AOI to be processed)\n"
     ]
    }
   ],
   "source": [
    "# Keep only tiles number that are both exists in the folder containing all LC .tif files and included in the AOI to be processed\n",
    "list_file = [(os.path.splitext(os.path.split(x)[-1])[0],x) for x in list_file if int(os.path.splitext(os.path.split(x)[-1])[0].split(\"_\")[-1]) in tiles_aoi]\n",
    "# Create list of cat to be processed\n",
    "list_cat = [x[0].split(\"_\")[-1] for x in list_file]\n",
    "print \"%s TIF files will be imported (contained in the AOI to be processed)\"%len(list_file)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Delete existing raster from previous versions processings\n",
    "for rast in list_file:\n",
    "    gscript.run_command('g.remove', flags='f', type='raster', name=rast[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import individual rasters (for each tile)\n",
    "start_import = start_processing()\n",
    "for rast in list_file:\n",
    "    gscript.run_command('r.in.gdal', overwrite=True, input=rast[1] , output=rast[0])\n",
    "print_processing_time(start_import, \"Import achieved in \")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create virtual raster\n",
    "gscript.run_command('r.buildvrt', overwrite=True, \n",
    "                    input=\",\".join([a[0] for a in list_file]), \n",
    "                    output='fusion_lc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply functions on each tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set number of cores to use\n",
    "ncores = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_cat = ['3932','3933','5293','5294','6325','6326']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Test worker on one specific tile\n",
    "Worker(list_cat[list_cat.index(\"3932\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Computation (on 20 cores) achieved in 1 minutes and 6.6 seconds'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Launch processes in parallel\n",
    "start_parallel = start_processing()\n",
    "p = Pool(ncores)\n",
    "output = p.map(Worker, list_cat[:])  # Launch the processes for as many items in the list (if function with a return, the returned results are ordered thanks to 'map' function)\n",
    "p.close()\n",
    "p.join()\n",
    "# Print\n",
    "print_processing_time(start_parallel, \"Computation (on %s cores) achieved in \"%ncores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check log file for ERRORS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 log files in the folder\n"
     ]
    }
   ],
   "source": [
    "# Get list of csv with classification feature of individual tiles\n",
    "import glob\n",
    "list_log = glob.glob(os.path.join(config_parameters['outputfolder_Logfile'],\"Log_Prediction_tile_*.txt\"))\n",
    "print \"%s log files in the folder\"%len(list_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tile(s) faced an ERROR during the processing.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Declare new counter\n",
    "count = 0\n",
    "# Declare new list that will contain list of tile with error\n",
    "tile_error_list = []\n",
    "# Loop on list of log file\n",
    "for logfile in list_log:\n",
    "    got_error = False\n",
    "    tile_num = os.path.splitext(os.path.basename(logfile))[0].split(\"_\")[-1]\n",
    "    fin = open(logfile, 'r')\n",
    "    for row in fin:\n",
    "        if row[:5] == \"ERROR\":  # If at least one line have error message, the whole file will be counted as 1 error\n",
    "            got_error = True\n",
    "    if got_error:    \n",
    "        count += 1\n",
    "        tile_error_list.append(tile_num)  # Add tile number to the list\n",
    "# Print\n",
    "print \"%s tile(s) faced an ERROR during the processing.\\n\"%count\n",
    "\n",
    "# Update tile list with only tiles that have ERROR in log \n",
    "print \"\\n\".join([\"Error on tile %s\"%(a) for a in tile_error_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create VRT with all raster products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Location 'WALOUS_31370' already exist\",\n",
       " \"'POSTPROCESS' mapset already exists in location 'WALOUS_31370'\",\n",
       " \"You are now working in mapset 'WALOUS_31370/POSTPROCESS'\"]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create mapset \n",
    "launch_mapset(\"POSTPROCESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate VRT\n",
    "gscript.run_command('r.buildvrt', overwrite=True, \n",
    "                    input=\",\".join([\"resamp@%s\"%cat for cat in list_cat]), \n",
    "                    output='resamp_vrt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate VRT\n",
    "gscript.run_command('r.buildvrt', overwrite=True, \n",
    "                    input=\",\".join([\"resamp_mmu@%s\"%cat for cat in list_cat]), \n",
    "                    output='resamp_mmu_vrt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract vectorial outputs at the 'commune' level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import layers for the whole wallonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run ../SRC/config_postprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Location 'WALOUS_31370' already exist\",\n",
       " \"'PERMANENT' mapset already exists in location 'WALOUS_31370'\",\n",
       " \"You are now working in mapset 'WALOUS_31370/PERMANENT'\"]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create mapset \n",
    "launch_mapset(\"PERMANENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Communes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import vector layer of the belgian communes\n",
    "gscript.run_command('v.import', overwrite=True, \n",
    "                    epsg='31370', input=data['communes'][1], output=data['communes'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of communes ID\n",
    "list_com = [code \n",
    "            for code \n",
    "            in gscript.read_command('v.db.select', flags='c', map=data['communes'][0], columns='cd_munty_r').split('\\n') \n",
    "            if len(code)>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cadastral blocks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import vector layer of the belgian communes\n",
    "gscript.run_command('v.import', overwrite=True, \n",
    "                    epsg='3812', input=data['CaPa'][1], output=data['CaPa'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract LC proportion by CaPa for each commune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of communes ID intersecting MARCHE area\n",
    "list_com = ['61012','83012','83028','83034','83040','91030','91064','91120']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define region based on the 1m spatial resolution raster LC output\n",
    "g.region raster=clip_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v.to.rast --overwrite input=CaPa output=CaPa use=attr attribute_column=cat label_column=CaPaKey memory=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def Clean(tile_cat):\n",
    "    #Declare empty string that will contain the messages to return\n",
    "    return_message = ''\n",
    "    try:\n",
    "        for layer in list_rast_fusion:\n",
    "            gscript.run_command('g.remove', flags='f', type=\"raster\", name=layer)\n",
    "        return_message += \"--> Mapset cleaned\"\n",
    "    except:\n",
    "        return_message += \"ERROR: during mapset cleaning. Please check for problem.\"\n",
    "    return return_message"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def Worker(tile_cat):\n",
    "    import subprocess\n",
    "    start_tile = start_processing() \n",
    "#    print \"Start processing on tile %s\"%tile_cat\n",
    "    #Declare empty list for saving output messages\n",
    "    output_message = [] \n",
    "    \n",
    "    # Launch mapset\n",
    "    message = launch_mapset(tile_cat)  \n",
    "    [output_message.append(a) for a in message]\n",
    "#    print \"\\n\".join(message)\n",
    "\n",
    "    # Allow access to other mapset \n",
    "    message = GetMapsetsAccess()\n",
    "    output_message.append(message)\n",
    "#    print message\n",
    "    \n",
    "    # Define computional region and mask\n",
    "    message = DefineComputationRegionAndMask(tile_cat,resamp_resolution='1', grow_mmu='16')\n",
    "    output_message.append(message)\n",
    "#    print message    \n",
    "\n",
    "    # Resample\n",
    "    message = Resamp(tile_cat)\n",
    "    output_message.append(message)\n",
    "#    print message \n",
    "\n",
    "    # ReclassArea according to MMU\n",
    "    message = ReclassArea(tile_cat, MMU='0.0015')\n",
    "    output_message.append(message)\n",
    "#    print message \n",
    "\n",
    "    # ColorizeRaster\n",
    "    message = ColorizeRaster(tile_cat)\n",
    "    output_message.append(message)\n",
    "#    print message \n",
    "\n",
    "    # Vectorize\n",
    "    message = Vectorize(tile_cat)\n",
    "    output_message.append(message)\n",
    "#    print message \n",
    "\n",
    "    # VectorGeneralize\n",
    "    message = VectorGeneralize(tile_cat, thresh_value=\"10000000\", iter_value=\"3\")\n",
    "    output_message.append(message)\n",
    "#    print message \n",
    "\n",
    "    # ClipToTileBorder\n",
    "    message = ClipToTileBorder(tile_cat)\n",
    "    output_message.append(message)\n",
    "#    print message \n",
    "\n",
    "    # ColorizeVector\n",
    "    message = ColorizeVector(tile_cat)\n",
    "    output_message.append(message)\n",
    "#    print message \n",
    "\n",
    "    # ExportRast \n",
    "    message = ExportFinalRast(tile_cat)\n",
    "    output_message.append(message)\n",
    "#    print message\n",
    "    \n",
    "    # ExportIntermediateRast \n",
    "    message = ExportIntermediateRast(tile_cat)\n",
    "    output_message.append(message)\n",
    "#    print message\n",
    "\n",
    "    # ExportVect \n",
    "    #message = ExportVect(tile_cat)\n",
    "    #output_message.append(message)\n",
    "#    print message\n",
    "\n",
    "    # ExportAllVect \n",
    "    message = ExportAllVect(tile_cat)\n",
    "    output_message.append(message)\n",
    "#    print message\n",
    "\n",
    "    # Clean \n",
    "    #message = Clean(tile_cat)\n",
    "    #output_message.append(message)\n",
    "#    print message\n",
    "\n",
    "    #Print processing time\n",
    "    message = print_processing_time(start_tile, \"Prediction for tile '%s' achieved in \"%tile_cat)\n",
    "    output_message.append(message)\n",
    "#    print message\n",
    "    \n",
    "    #Export Log file\n",
    "    fout = open(os.path.join(config_parameters['outputfolder_Logfile'],\"Log_Prediction_tile_%s.txt\"%tile_cat),\"w\")\n",
    "    [fout.writelines('%s\\n'%content) for content in output_message]\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract polygons for each 'commune'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v.db.addcolumn map=test_overlay@3219 columns=\"class integer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v.db.update map=test_overlay@3219 column=class query_column=a_class where=\"b_class is null\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v.db.update map=test_overlay@3219 column=class query_column=a_class where=\"(a_class is not null) AND (b_class is not null) \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v.dissolve input=test_overlay@3219 column=class output=test_dissolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
